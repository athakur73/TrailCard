{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06238f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5169e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPatient First Name: William\\nPatient Last Name: Hicks\\nRx#: 210280140357\\nDrug Name: INVOKANA TABS\\nNDC: 50458-0141-90\\nFilled date: 09/16/2021\\nDay supply: \\nQuantity: 90\\nAmount:\\nPrimary Paid Amount:\\nCopay: $120.00\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Patient First Name: William\n",
    "Patient Last Name: Hicks\n",
    "Rx#: 210280140357\n",
    "Drug Name: INVOKANA TABS\n",
    "NDC: 50458-0141-90\n",
    "Filled date: 09/16/2021\n",
    "Day supply: \n",
    "Quantity: 90\n",
    "Amount:\n",
    "Primary Paid Amount:\n",
    "Copay: $120.00\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2121870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(text.replace('\\n',' ').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc558e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nameRegex = re.compile(r\"^[a-zA-Z]+[',. -][a-zA-Z ]?[a-zA-Z]*$\", re.MULTILINE)\n",
    "# nameRegex = re.compile('^((?:\\w+\\s*){2,})$', flags=re.MULTILINE)\n",
    "\n",
    "# print(nameRegex.findall(text15.replace('\\n',' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e608ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(r\"[^()0-9-]+\", text15.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c1053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text15.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfdb9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.findall(r\"[^()0-9-]+\", ''.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d537a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import re\n",
    "import os\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "path = r\"C:\\Users\\I4724\\Downloads\\Trialcard Docs\\docs\"\n",
    "dir_list = os.listdir(path)\n",
    "\n",
    "\n",
    "data = dict()\n",
    "for file in dir_list:\n",
    "    mx = ''\n",
    "    mx = pytesseract.image_to_string(Image.open(path+'\\\\'+file))\n",
    "    mx = mx.replace('\\n\\n',' ')\n",
    "    mx = mx.replace('\\n',' ')\n",
    "    res = []\n",
    "    for x in mx.split():\n",
    "        st = ' '.join([w for w in x.split() if w not in stopwords])\n",
    "        res.append(st)\n",
    "    data[file.split('.')[0]] = ' '.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fc383bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fef00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac6d393a",
   "metadata": {},
   "source": [
    "## Using spacy PhaseMatcher for extracting name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e8cd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00308316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = nlp(data['15'])\n",
    "\n",
    "# for ent in docs.ents:\n",
    "#     print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb66a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = [\n",
    "\"KATHLEEN A. MCGRAW\",\n",
    "\"HOERBER, STEVEN\",\n",
    "\"HA, ETHAN\",\n",
    "\"Richard Roder\",\n",
    "\"NEITZEL RANDALL,R\",\n",
    "\"Gross, Margaret\",\n",
    "\"Sarge, Maureen\",\n",
    "\"FERNANDO FLORES\",\n",
    "\"CYNTHIA DAFFIN\",\n",
    "\"Suzette\",\n",
    "\"CRAIG HILER\",\n",
    "\"Madrigal, Graciela\",\n",
    "\"Lombardo, Christopher\",\n",
    "\"DANIEL ROMSTEDT\",\n",
    "\"WILLIAM HICKS\",\n",
    "\"BRIAN CARLEW\", \n",
    "\"JOSEPH HINNENKAMP\",\n",
    "\"Scarano, Vincent\",\n",
    "\"SAMUEL CASTANEDA\",\n",
    "\"HOWARD JESSEE\",\n",
    "\"PAUL C VIEYRA\",\n",
    "\"Randles, David\",\n",
    "\"RONALD NALDOZA\",\n",
    "\"SUZANNE CHEANG\",\n",
    "\"BELL, LINDA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6873c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa076d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_div(name):\n",
    "    if ',' in name:\n",
    "        fname = name.split(',')[1].lstrip(' ')\n",
    "        lname = name.split(',')[0]\n",
    "        \n",
    "    else:\n",
    "        fname = name.split()[0]\n",
    "        lname = name.split()[len(name.split())-1]\n",
    "    return {'First Name':fname.title(), 'Last Name':lname.title()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775c5b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 1 - {'First Name': 'Kathleen', 'Last Name': 'Mcgraw'}\n",
      "doc 2 - {'First Name': 'Steven', 'Last Name': 'Hoerber'}\n",
      "doc 3 - {'First Name': 'Ethan', 'Last Name': 'Ha'}\n",
      "doc 4 - {'First Name': 'Richard', 'Last Name': 'Roder'}\n",
      "doc 5 - Name: Not able to extract name from the document\n",
      "doc 6 - Name: Not able to extract name from the document\n",
      "doc 7 - Name: Not able to extract name from the document\n",
      "doc 8 - {'First Name': 'Fernando', 'Last Name': 'Flores'}\n",
      "doc 9 - {'First Name': 'Cynthia', 'Last Name': 'Daffin'}\n",
      "doc 11 - Name: Not able to extract name from the document\n",
      "doc 12 - Name: Not able to extract name from the document\n",
      "doc 13 - Name: Not able to extract name from the document\n",
      "doc 14 - {'First Name': 'Daniel', 'Last Name': 'Romstedt'}\n",
      "doc 15 - {'First Name': 'William', 'Last Name': 'Hicks'}\n",
      "doc 16 - {'First Name': 'Brian', 'Last Name': 'Carlew'}\n",
      "doc 17 - {'First Name': 'Joseph', 'Last Name': 'Hinnenkamp'}\n",
      "doc 18 - {'First Name': 'Vincent', 'Last Name': 'Scarano'}\n",
      "doc 19 - {'First Name': 'Samuel', 'Last Name': 'Castaneda'}\n",
      "doc 20 - {'First Name': 'Howard', 'Last Name': 'Jessee'}\n",
      "doc 21 - {'First Name': 'Paul', 'Last Name': 'Vieyra'}\n",
      "doc 22 - Name: Not able to extract name from the document\n",
      "doc 23 - {'First Name': 'Ronald', 'Last Name': 'Naldoza'}\n",
      "doc 24 - {'First Name': 'Suzanne', 'Last Name': 'Cheang'}\n",
      "doc 25 - Name: Not able to extract name from the document\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "#terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "for i in range(1,26):\n",
    "    if(i!=10):\n",
    "        doc = nlp(data[str(i)])\n",
    "        matches = matcher(doc)\n",
    "        if(len(matches)>0):\n",
    "            for match_id, start, end in matches:\n",
    "                span = doc[start:end]\n",
    "            #print(span.text)\n",
    "\n",
    "            print(f'doc {i} - {name_div(span.text)}')\n",
    "        else:\n",
    "            print(f'doc {i} - Name: Not able to extract name from the document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ce626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# translate_table = dict((ord(char), None) for char in string.punctuation)   \n",
    "# mx = data['9'].translate(translate_table)\n",
    "# mx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb951f17",
   "metadata": {},
   "source": [
    "## Extracting NDC using regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98f896fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['NDC  50458057930']\n",
      "['NDC 00781300407']\n",
      "['NDC ']\n",
      "['NDC 50458057930']\n",
      "[]\n",
      "['NDC 00069154068']\n",
      "['NDC ']\n",
      "['NDC50458014030', 'NDC5045801403']\n",
      "[]\n",
      "['NDC 66220072930']\n",
      "[]\n",
      "[]\n",
      "['NDC  504580141900']\n",
      "['NDC 00074659419', 'NDC 50458014190']\n",
      "['NDC 00013830304']\n",
      "[]\n",
      "[]\n",
      "['NDC ']\n",
      "[]\n",
      "['NDC50459']\n",
      "['NDC 00069154068']\n",
      "['NDC  61958230101']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)  \n",
    "pattern = 'NDC\\s*[0-9]*' \n",
    "All_NDC = []\n",
    "for i in range(1,26):\n",
    "    if(i!=10):\n",
    "        All_NDC.append(re.findall(pattern,data[str(i)].translate(translate_table)))\n",
    "        print(re.findall(pattern,data[str(i)].translate(translate_table)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e79716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "# pattern = '[0-9]{11}' \n",
    "# All_NDC = []\n",
    "# for i in range(1,26):\n",
    "#     if(i!=10):\n",
    "#         #All_NDC.append(re.findall(pattern,data[str(i)].translate(translate_table)))\n",
    "#         print(re.findall(pattern,data[str(i)].translate(translate_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf7f2f9",
   "metadata": {},
   "source": [
    "## Extracting date from the extracted text from each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16989196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def max_Date(dat):\n",
    "    all_dates = []\n",
    "    sy = int(dat[0].split('/')[2])\n",
    "    for d in dat:\n",
    "        mx = d.split('/')\n",
    "        year = int(mx[2])\n",
    "        month = int(mx[0])\n",
    "        day = int(mx[1])\n",
    "        dates = date(year, month, day)\n",
    "        if(abs(year-sy)<2):\n",
    "            all_dates.append(dates)\n",
    "        \n",
    "    year = min(all_dates).year\n",
    "    month = min(all_dates).month\n",
    "    day = min(all_dates).day\n",
    "\n",
    "    return [str(month)+'/'+str(day)+'/'+str(year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f42fbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11/03/2021'],\n",
       " ['11/15/2021'],\n",
       " ['09/20/21'],\n",
       " ['9/26/21'],\n",
       " ['11/04/24'],\n",
       " ['8/14/21'],\n",
       " ['3/22/22'],\n",
       " ['11/01/2021'],\n",
       " ['10/20/21'],\n",
       " ['12/01/2020'],\n",
       " ['10/12/21'],\n",
       " [],\n",
       " ['3/31/2018'],\n",
       " ['9/16/2021'],\n",
       " ['9/16/21'],\n",
       " ['9/16/21'],\n",
       " ['2/3/21'],\n",
       " [],\n",
       " ['10/8/2021'],\n",
       " [],\n",
       " [],\n",
       " ['9/18/21'],\n",
       " ['10/14/21'],\n",
       " ['6/18/21']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "pattern = '\\d+/\\d+/\\d+' \n",
    "All_Date = []\n",
    "for i in range(1,26):\n",
    "    if(i!=10):\n",
    "        All_Date.append(list(set(re.findall(pattern,data[str(i)]))))\n",
    "\n",
    "for i in range(len(All_Date)):\n",
    "    if(len(All_Date[i])>1):\n",
    "        All_Date[i] = max_Date(All_Date[i])\n",
    "All_Date        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3fb17a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11/03/2021'],\n",
       " ['11/15/2021'],\n",
       " ['09/20/21'],\n",
       " ['9/26/21'],\n",
       " ['11/04/24'],\n",
       " ['8/14/21'],\n",
       " ['3/22/22'],\n",
       " ['11/01/2021'],\n",
       " ['10/20/21'],\n",
       " ['12/01/2020'],\n",
       " ['10/12/21'],\n",
       " [],\n",
       " ['9/13/2021'],\n",
       " ['9/16/2021'],\n",
       " ['9/16/21'],\n",
       " ['9/16/21'],\n",
       " ['2/3/21'],\n",
       " [],\n",
       " ['10/8/2021'],\n",
       " [],\n",
       " [],\n",
       " ['9/22/2021'],\n",
       " ['10/14/2021'],\n",
       " ['6/18/21']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "pattern = '\\d+/\\d+/\\d+' \n",
    "All_Date = []\n",
    "for i in range(1,26):\n",
    "    if(i!=10):\n",
    "        All_Date.append(list(set(re.findall(pattern,data[str(i)]))))\n",
    "\n",
    "for i in range(len(All_Date)):\n",
    "    if(len(All_Date[i])>1):\n",
    "        All_Date[i] = max_Date(All_Date[i])\n",
    "All_Date  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50bab7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['11/03/2021'],\n",
       " ['11/15/2021', '06/16/2022'],\n",
       " ['09/20/21'],\n",
       " ['9/26/21'],\n",
       " ['11/04/24'],\n",
       " ['8/14/21', '11/17/21'],\n",
       " ['3/22/22'],\n",
       " ['11/01/2021'],\n",
       " ['08/17/22', '10/20/21'],\n",
       " ['12/01/2020'],\n",
       " ['10/12/21'],\n",
       " [],\n",
       " ['9/13/2021', '03/31/2018'],\n",
       " ['09/16/2021', '11/10/2021', '01/21/2022'],\n",
       " ['9/16/21', '12/25/2021', '09/27/2021', '08/16/2022', '05/24/2022'],\n",
       " ['9/16/21', '04/12/2022', '09/26/2021', '11/24/2021'],\n",
       " ['2/3/21'],\n",
       " [],\n",
       " ['10/19/2021', '10/08/2021', '10/20/2021'],\n",
       " [],\n",
       " [],\n",
       " ['05/04/2022', '09/22/2021', '9/18/21', '11/5/2021'],\n",
       " ['10/07/2022', '10/14/21', '10/14/2021', '12/27/2021'],\n",
       " ['06/18/22', '06/18/21']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pattern = '[0-9]+[\\s\\/]+[0-9]+[\\s\\/]+[0-9]+'\n",
    "All_Date = []\n",
    "for i in range(1,26):\n",
    "    if(i!=10):\n",
    "        All_Date.append(list(set(re.findall(pattern,data[str(i)]))))\n",
    "\n",
    "# for i in range(len(All_Date)):\n",
    "#     if(len(All_Date[i])>1):\n",
    "#         All_Date[i] = max_Date(All_Date[i])\n",
    "All_Date    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "adb6d4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'30 Fill Dated Written: 06/18/21 BELL 1  1 S+OLD ELIZARETHTOWN . PA 17022 PHONE 8.9745 DOB: 12/09 ILENOR 6MG T. BG#42847-0106-30 RNIX THERAPEUTICS Days Supply: 30 Qty: 30 3.00 Refills  06/18/22 Retail Price: 3 REFILL Fill # 02 47164721300200601 4100486210004000100'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116be8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['09/20/21']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(All_Date[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e38b12",
   "metadata": {},
   "source": [
    "## Quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate_table = dict((ord(char), None) for char in string.punctuation) \n",
    "# pattern = 'Q[a-zA-Z]*\\s*[0-9]*' \n",
    "# for i in range(1,26):\n",
    "#     if(i!=10):\n",
    "#         print(re.findall(pattern,data[str(i)].translate(translate_table)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a042cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049153a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983c05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fff13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
